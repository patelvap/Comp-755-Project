{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LDA2Vec",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS5jhdJbp98C"
      },
      "source": [
        "# LDA2Vec\n",
        "Adapted from tensorflow implementation: \n",
        "\n",
        "(New) https://github.com/nateraw/Lda2vec-Tensorflow\n",
        "\n",
        "(Old) https://github.com/meereeum/lda2vec-tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yQ-mZTEqfJG"
      },
      "source": [
        "Import Libraries (Must restart after installing pyLDAvis)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "id": "SplvN5uwp7XK",
        "outputId": "93dcebb5-35ec-4d98-ec26-8d68286edc26"
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 81 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 337 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 348 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 358 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 368 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 378 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 389 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 399 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 409 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 419 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 430 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 440 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 450 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 460 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 471 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 481 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 491 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 501 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 512 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 522 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 532 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 542 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 552 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 563 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 573 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 583 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 593 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 604 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 614 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 624 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 634 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 645 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 655 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 665 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 675 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 686 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 696 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 706 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 716 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 727 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 737 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 747 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 757 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 768 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 778 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 788 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 798 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 808 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 819 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 829 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 839 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 849 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 860 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 870 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 880 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 890 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 901 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 911 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 921 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 931 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 942 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 952 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 962 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 972 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 983 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 993 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 5.3 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Collecting numpy>=1.20.0\n",
            "  Downloading numpy-1.21.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 199 kB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.1)\n",
            "Collecting pandas>=1.2.0\n",
            "  Downloading pandas-1.3.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 36.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.16-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.22.2.post1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.7.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136897 sha256=62d7faf3364e53426c2a1852f0609e845b5cc6d8ace085346f387edc6c8592ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: numpy, pandas, funcy, pyLDAvis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed funcy-1.16 numpy-1.21.2 pandas-1.3.3 pyLDAvis-3.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1HBXFOPqJOj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c40e72f-87ac-44b7-dda2-8e063eb4b8f0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aByKtIjvqkJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f05562c-c6ed-49f4-81d7-af1817a3ffc6"
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FzVnEHYrKP1"
      },
      "source": [
        "!cp -r /content/drive/MyDrive/Project/Lda2vec-Tensorflow-master .\n",
        "!cp /content/drive/MyDrive/Project/cleaned.txt .\n",
        "!cp /content/drive/MyDrive/Project/papers.csv ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfl9g1ze76ZO"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/Lda2vec-Tensorflow-master')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Apc0J1AqXEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e10ef20-f40c-48e0-aa45-3fd3254d7c4d"
      },
      "source": [
        "from lda2vec import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n",
            "/usr/local/lib/python3.7/dist-packages/past/builtins/misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Mapping\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:169: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:286: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:858: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True):\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1094: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1120: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, positive=False):\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1349: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1590: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_least_angle.py:1723: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  EPS = np.finfo(np.float).eps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc9Dtw23LCZW"
      },
      "source": [
        "## Find Number of Documents with Each Year"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLxxcVZILKU8"
      },
      "source": [
        "data = pd.read_csv(\"papers.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l7eKMuzLQzQ"
      },
      "source": [
        "data = data.sort_values(by=[\"year\"], kind='mergesort')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG82lAG-LTHM"
      },
      "source": [
        "years = list(data[\"year\"].unique())\n",
        "counts = list(data[\"year\"].value_counts())[::-1]\n",
        "for i in range(1,len(counts)):\n",
        "  counts[i] += counts[i-1]\n",
        "year_index = {years[i]:counts[i] for i in range(len(years))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMSZIaUhRHgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c369772d-169b-42dc-8dbb-e556f4ce90b0"
      },
      "source": [
        "year_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1987: 90,\n",
              " 1988: 184,\n",
              " 1989: 285,\n",
              " 1990: 412,\n",
              " 1991: 552,\n",
              " 1992: 695,\n",
              " 1993: 839,\n",
              " 1994: 989,\n",
              " 1995: 1139,\n",
              " 1996: 1290,\n",
              " 1997: 1442,\n",
              " 1998: 1594,\n",
              " 1999: 1746,\n",
              " 2000: 1904,\n",
              " 2001: 2101,\n",
              " 2002: 2299,\n",
              " 2003: 2503,\n",
              " 2004: 2710,\n",
              " 2005: 2917,\n",
              " 2006: 3124,\n",
              " 2007: 3341,\n",
              " 2008: 3591,\n",
              " 2009: 3853,\n",
              " 2010: 4145,\n",
              " 2011: 4451,\n",
              " 2012: 4811,\n",
              " 2013: 5179,\n",
              " 2014: 5582,\n",
              " 2015: 5993,\n",
              " 2016: 6562,\n",
              " 2017: 7241}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUjvC2BluF1s"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fTVmdGwuHrv"
      },
      "source": [
        "df = pd.read_csv(\"cleaned.txt\")\n",
        "df = df[df[\"stop_removed_paper_text\"].notnull()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkyneGh_RHDq"
      },
      "source": [
        "df = df[6562:7241] # Use only 2017 papers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FFPTwywuXJe"
      },
      "source": [
        "# Initialize a preprocessor\n",
        "P = nlppipe.Preprocessor(df, \"stop_removed_paper_text\", max_features=30000, maxlen=10000, min_count=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeW0xp3dvDyo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba099aa0-1597-4417-e27a-17126d664c19"
      },
      "source": [
        "# Run the preprocessing on your dataframe\n",
        "P.preprocess()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------- Tokenizing Texts ----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "676it [01:12,  9.29it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing 39328 low frequency tokens out of 45170 total tokens\n",
            "\n",
            "---------- Getting Skipgrams ----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "676it [00:59, 11.33it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y2VxT6kvHKH"
      },
      "source": [
        "# Save data to data_dir\n",
        "P.save_data(\"clean_data\", embedding_matrix=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzdVCxyOSd-s"
      },
      "source": [
        "## Using the LDA2Vec Model\n",
        "\n",
        "Use separate model if you wish to test on other sets of documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tjWK7FiSgw7"
      },
      "source": [
        "# Path to preprocessed data\n",
        "data_path  = \"clean_data\"\n",
        "# Whether or not to load saved embeddings file\n",
        "load_embeds = True\n",
        "\n",
        "# Load data from files\n",
        "(idx_to_word, word_to_idx, freqs, pivot_ids,\n",
        " target_ids, doc_ids) = utils.load_preprocessed_data(data_path, load_embed_matrix=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EtoUS_dSj6E"
      },
      "source": [
        "# Number of unique documents\n",
        "num_docs = doc_ids.max() + 1\n",
        "# Number of unique words in vocabulary (int)\n",
        "vocab_size = len(freqs)\n",
        "# Number of topics to cluster into\n",
        "num_topics = 20\n",
        "# Amount of iterations over entire dataset\n",
        "num_epochs = 100\n",
        "# Batch size - Increase/decrease depending on memory usage\n",
        "batch_size = 4096\n",
        "# Epoch that we want to \"switch on\" LDA loss\n",
        "switch_loss_epoch = 0\n",
        "# If True, save logdir, otherwise don't\n",
        "save_graph = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx68KWfkS6sd"
      },
      "source": [
        "# Initialize the model\n",
        "m = model(num_docs,\n",
        "          vocab_size,\n",
        "          num_topics,\n",
        "          embedding_size=128,\n",
        "          pretrained_embeddings=None,\n",
        "          freqs=freqs,\n",
        "          batch_size = batch_size,\n",
        "          save_graph_def=save_graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAHda-IRTQvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a451f1ac-8c34-477e-ae52-6579b1a435a4"
      },
      "source": [
        "# Train the model\n",
        "m.train(pivot_ids,\n",
        "        target_ids,\n",
        "        doc_ids,\n",
        "        len(pivot_ids),\n",
        "        num_epochs,\n",
        "        idx_to_word=idx_to_word,\n",
        "        switch_loss_epoch=switch_loss_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH: 1\n",
            "LOSS 1941.62 w2v 4.8553553 lda 1936.7646\n",
            "\n",
            "EPOCH: 2\n",
            "LOSS 1924.3671 w2v 4.7416286 lda 1919.6255\n",
            "\n",
            "EPOCH: 3\n",
            "LOSS 1924.8082 w2v 5.187755 lda 1919.6205\n",
            "\n",
            "EPOCH: 4\n",
            "LOSS 1924.6147 w2v 4.994253 lda 1919.6205\n",
            "\n",
            "EPOCH: 5\n",
            "LOSS 1927.3643 w2v 7.743763 lda 1919.6205\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "---------Closest 10 words to given indexes----------\n",
            "Topic 0 : particular, similar, specifically, contrast, need, based, approach, example, addition, finally\n",
            "Topic 1 : particular, similar, specifically, contrast, need, based, approach, example, addition, finally\n",
            "Topic 2 : particular, similar, specifically, contrast, need, based, approach, addition, example, finally\n",
            "Topic 3 : particular, similar, specifically, need, contrast, based, approach, example, addition, finally\n",
            "Topic 4 : particular, similar, specifically, contrast, need, based, approach, example, addition, finally\n",
            "Topic 5 : particular, similar, specifically, contrast, need, based, approach, example, addition, finally\n",
            "Topic 6 : particular, similar, specifically, contrast, need, based, approach, example, addition, finally\n",
            "Topic 7 : particular, similar, specifically, contrast, need, based, approach, addition, example, finally\n",
            "Topic 8 : particular, similar, specifically, contrast, need, based, approach, example, addition, finally\n",
            "Topic 9 : particular, similar, specifically, contrast, need, based, approach, example, addition, finally\n",
            "Topic 10 : particular, similar, specifically, contrast, need, based, approach, addition, example, finally\n",
            "Topic 11 : particular, similar, specifically, contrast, need, based, approach, addition, example, finally\n",
            "Topic 12 : particular, similar, specifically, contrast, need, based, approach, example, addition, finally\n",
            "Topic 13 : particular, similar, specifically, contrast, need, based, approach, example, addition, finally\n",
            "Topic 14 : particular, similar, specifically, contrast, need, based, approach, example, addition, finally\n",
            "Topic 15 : particular, similar, specifically, contrast, need, based, approach, example, addition, finally\n",
            "Topic 16 : particular, similar, specifically, contrast, need, based, approach, example, addition, finally\n",
            "Topic 17 : particular, similar, specifically, contrast, need, based, approach, example, addition, finally\n",
            "Topic 18 : particular, similar, specifically, contrast, need, based, approach, example, addition, finally\n",
            "Topic 19 : particular, similar, specifically, contrast, need, based, approach, addition, example, finally\n",
            "\n",
            "EPOCH: 6\n",
            "LOSS 1924.4814 w2v 4.860918 lda 1919.6205\n",
            "\n",
            "EPOCH: 7\n",
            "LOSS 1924.642 w2v 5.0214934 lda 1919.6205\n",
            "\n",
            "EPOCH: 8\n",
            "LOSS 1924.1488 w2v 4.5282726 lda 1919.6205\n",
            "\n",
            "EPOCH: 9\n",
            "LOSS 1924.8186 w2v 5.1981363 lda 1919.6205\n",
            "\n",
            "EPOCH: 10\n",
            "LOSS 1924.2852 w2v 4.66464 lda 1919.6205\n",
            "---------Closest 10 words to given indexes----------\n",
            "Topic 0 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 1 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 2 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 3 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 4 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 5 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 6 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 7 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 8 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 9 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 10 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 11 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 12 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 13 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 14 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 15 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 16 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 17 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 18 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "Topic 19 : particular, addition, similar, based, specifically, finally, need, resulting, example, approach\n",
            "\n",
            "EPOCH: 11\n",
            "LOSS 1924.9733 w2v 5.352819 lda 1919.6205\n",
            "\n",
            "EPOCH: 12\n",
            "LOSS 1925.6947 w2v 6.0742598 lda 1919.6205\n",
            "\n",
            "EPOCH: 13\n",
            "LOSS 1925.1292 w2v 5.5086613 lda 1919.6205\n",
            "\n",
            "EPOCH: 14\n",
            "LOSS 1924.236 w2v 4.61545 lda 1919.6205\n",
            "\n",
            "EPOCH: 15\n",
            "LOSS 1924.4241 w2v 4.80354 lda 1919.6205\n",
            "---------Closest 10 words to given indexes----------\n",
            "Topic 0 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 1 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 2 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 3 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 4 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 5 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 6 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 7 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 8 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 9 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 10 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 11 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 12 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 13 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 14 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 15 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 16 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 17 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 18 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "Topic 19 : particular, addition, similar, note, specifically, based, finally, example, resulting, need\n",
            "\n",
            "EPOCH: 16\n",
            "LOSS 1925.4735 w2v 5.8530526 lda 1919.6205\n",
            "\n",
            "EPOCH: 17\n",
            "LOSS 1924.536 w2v 4.9157495 lda 1919.6202\n",
            "\n",
            "EPOCH: 18\n",
            "LOSS 1924.2014 w2v 4.5809393 lda 1919.6205\n",
            "\n",
            "EPOCH: 19\n",
            "LOSS 1927.1307 w2v 7.510516 lda 1919.6202\n",
            "\n",
            "EPOCH: 20\n",
            "LOSS 1924.1333 w2v 4.5127935 lda 1919.6205\n",
            "---------Closest 10 words to given indexes----------\n",
            "Topic 0 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 1 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 2 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 3 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 4 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 5 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 6 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 7 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 8 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 9 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 10 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 11 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 12 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 13 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 14 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 15 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 16 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 17 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 18 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "Topic 19 : particular, addition, note, similar, specifically, finally, example, based, resulting, instead\n",
            "\n",
            "EPOCH: 21\n",
            "LOSS 1924.251 w2v 4.630509 lda 1919.6205\n",
            "\n",
            "EPOCH: 22\n",
            "LOSS 1925.6288 w2v 6.008314 lda 1919.6205\n",
            "\n",
            "EPOCH: 23\n",
            "LOSS 1928.0475 w2v 8.426945 lda 1919.6205\n",
            "\n",
            "EPOCH: 24\n",
            "LOSS 1924.619 w2v 4.9985185 lda 1919.6205\n",
            "\n",
            "EPOCH: 25\n",
            "LOSS 1925.1783 w2v 5.557808 lda 1919.6205\n",
            "---------Closest 10 words to given indexes----------\n",
            "Topic 0 : particular, note, addition, similar, specifically, example, finally, based, resulting, instead\n",
            "Topic 1 : particular, note, addition, similar, specifically, example, finally, based, resulting, instead\n",
            "Topic 2 : particular, note, addition, similar, specifically, example, finally, based, instead, resulting\n",
            "Topic 3 : particular, note, addition, similar, specifically, example, finally, based, resulting, instead\n",
            "Topic 4 : particular, note, addition, similar, specifically, example, finally, based, resulting, instead\n",
            "Topic 5 : particular, note, addition, similar, specifically, example, finally, based, resulting, instead\n",
            "Topic 6 : particular, note, addition, similar, specifically, example, finally, based, resulting, instead\n",
            "Topic 7 : particular, note, addition, similar, specifically, example, finally, based, resulting, instead\n",
            "Topic 8 : particular, note, addition, similar, specifically, example, finally, based, resulting, instead\n",
            "Topic 9 : particular, note, addition, similar, specifically, example, finally, based, resulting, instead\n",
            "Topic 10 : particular, note, addition, similar, specifically, example, finally, based, instead, resulting\n",
            "Topic 11 : particular, note, addition, similar, specifically, example, finally, based, resulting, instead\n",
            "Topic 12 : particular, note, addition, similar, specifically, example, finally, based, resulting, instead\n",
            "Topic 13 : particular, note, addition, similar, specifically, example, finally, based, resulting, instead\n",
            "Topic 14 : particular, note, addition, similar, specifically, example, finally, based, instead, resulting\n",
            "Topic 15 : particular, note, addition, similar, specifically, example, finally, based, resulting, instead\n",
            "Topic 16 : particular, note, addition, similar, specifically, example, finally, based, resulting, instead\n",
            "Topic 17 : particular, note, addition, similar, specifically, example, finally, based, resulting, instead\n",
            "Topic 18 : particular, note, addition, similar, specifically, example, finally, based, resulting, instead\n",
            "Topic 19 : particular, note, addition, similar, specifically, example, finally, based, resulting, instead\n",
            "\n",
            "EPOCH: 26\n",
            "LOSS 1925.4869 w2v 5.8665085 lda 1919.6205\n",
            "\n",
            "EPOCH: 27\n",
            "LOSS 1924.4929 w2v 4.8726788 lda 1919.6202\n",
            "\n",
            "EPOCH: 28\n",
            "LOSS 1924.1741 w2v 4.553617 lda 1919.6205\n",
            "\n",
            "EPOCH: 29\n",
            "LOSS 1924.5402 w2v 4.9197025 lda 1919.6205\n",
            "\n",
            "EPOCH: 30\n",
            "LOSS 1925.4075 w2v 5.7870073 lda 1919.6205\n",
            "---------Closest 10 words to given indexes----------\n",
            "Topic 0 : particular, note, addition, specifically, similar, example, finally, instead, based, given\n",
            "Topic 1 : particular, note, addition, similar, specifically, example, finally, instead, based, given\n",
            "Topic 2 : particular, note, addition, similar, specifically, example, finally, instead, based, given\n",
            "Topic 3 : particular, note, addition, specifically, similar, example, finally, instead, based, given\n",
            "Topic 4 : particular, note, addition, similar, specifically, example, finally, instead, based, given\n",
            "Topic 5 : particular, note, addition, similar, specifically, example, finally, instead, based, given\n",
            "Topic 6 : particular, note, addition, similar, specifically, example, finally, instead, based, given\n",
            "Topic 7 : particular, note, addition, similar, specifically, example, finally, instead, based, given\n",
            "Topic 8 : particular, note, addition, similar, specifically, example, finally, instead, based, given\n",
            "Topic 9 : particular, note, addition, similar, specifically, example, finally, instead, based, given\n",
            "Topic 10 : particular, note, addition, similar, specifically, example, finally, instead, based, given\n",
            "Topic 11 : particular, note, addition, similar, specifically, example, finally, instead, based, given\n",
            "Topic 12 : particular, note, addition, specifically, similar, example, finally, instead, based, given\n",
            "Topic 13 : particular, note, addition, similar, specifically, example, finally, instead, based, given\n",
            "Topic 14 : particular, note, addition, similar, specifically, example, finally, instead, based, given\n",
            "Topic 15 : particular, note, addition, similar, specifically, example, finally, instead, based, given\n",
            "Topic 16 : particular, note, addition, specifically, similar, example, finally, instead, based, given\n",
            "Topic 17 : particular, note, addition, similar, specifically, example, finally, instead, based, given\n",
            "Topic 18 : particular, note, addition, specifically, similar, example, finally, instead, based, given\n",
            "Topic 19 : particular, note, addition, specifically, similar, example, finally, instead, based, given\n",
            "\n",
            "EPOCH: 31\n",
            "LOSS 1924.3986 w2v 4.7781105 lda 1919.6205\n",
            "\n",
            "EPOCH: 32\n",
            "LOSS 1924.9127 w2v 5.2922683 lda 1919.6205\n",
            "\n",
            "EPOCH: 33\n",
            "LOSS 1925.7162 w2v 6.0957236 lda 1919.6205\n",
            "\n",
            "EPOCH: 34\n",
            "LOSS 1924.825 w2v 5.2044435 lda 1919.6205\n",
            "\n",
            "EPOCH: 35\n",
            "LOSS 1924.7709 w2v 5.1503396 lda 1919.6205\n",
            "---------Closest 10 words to given indexes----------\n",
            "Topic 0 : note, particular, addition, specifically, similar, example, instead, given, finally, based\n",
            "Topic 1 : note, particular, addition, specifically, similar, example, instead, finally, given, based\n",
            "Topic 2 : note, particular, addition, specifically, similar, example, instead, given, finally, based\n",
            "Topic 3 : note, particular, addition, specifically, similar, example, instead, given, finally, based\n",
            "Topic 4 : note, particular, addition, specifically, similar, example, instead, finally, given, based\n",
            "Topic 5 : note, particular, addition, specifically, similar, example, instead, given, finally, based\n",
            "Topic 6 : note, particular, addition, specifically, similar, example, instead, given, finally, based\n",
            "Topic 7 : note, particular, addition, specifically, similar, example, instead, finally, given, based\n",
            "Topic 8 : note, particular, addition, specifically, similar, example, instead, finally, given, based\n",
            "Topic 9 : note, particular, addition, specifically, similar, example, instead, given, finally, based\n",
            "Topic 10 : note, particular, addition, specifically, similar, example, instead, finally, given, based\n",
            "Topic 11 : note, particular, addition, specifically, similar, example, instead, finally, given, based\n",
            "Topic 12 : note, particular, addition, specifically, similar, example, instead, given, finally, based\n",
            "Topic 13 : note, particular, addition, specifically, similar, example, instead, finally, given, based\n",
            "Topic 14 : note, particular, addition, specifically, similar, example, instead, given, finally, based\n",
            "Topic 15 : note, particular, addition, specifically, similar, example, instead, given, finally, based\n",
            "Topic 16 : note, particular, addition, specifically, similar, example, instead, given, finally, based\n",
            "Topic 17 : note, particular, addition, specifically, similar, example, instead, given, finally, based\n",
            "Topic 18 : note, particular, addition, specifically, similar, example, instead, given, finally, based\n",
            "Topic 19 : note, particular, addition, specifically, similar, example, instead, given, finally, based\n",
            "\n",
            "EPOCH: 36\n",
            "LOSS 1926.1464 w2v 6.525937 lda 1919.6205\n",
            "\n",
            "EPOCH: 37\n",
            "LOSS 1924.9185 w2v 5.298027 lda 1919.6205\n",
            "\n",
            "EPOCH: 38\n",
            "LOSS 1925.4528 w2v 5.8323107 lda 1919.6205\n",
            "\n",
            "EPOCH: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2itPKBHwEmyf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
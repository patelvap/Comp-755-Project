{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eS5jhdJbp98C"
   },
   "source": [
    "# LDA2Vec\n",
    "Adapted from tensorflow implementation: \n",
    "\n",
    "(Current) https://github.com/nateraw/Lda2vec-Tensorflow\n",
    "\n",
    "(Old) https://github.com/meereeum/lda2vec-tf\n",
    "\n",
    "(Original) https://github.com/cemoody/lda2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1HBXFOPqJOj",
    "outputId": "6ee5f2d9-9b99-4dfc-f470-d11f82c38409"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyLDAvis\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sfl9g1ze76ZO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./Lda2vec-Tensorflow-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-Apc0J1AqXEb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\matth\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from lda2vec import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "#device_name = tf.test.gpu_device_name()\n",
    "#if device_name != '/device:GPU:0':\n",
    "#  raise SystemError('GPU device not found')\n",
    "#print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vc9Dtw23LCZW"
   },
   "source": [
    "## Find Ending Index of Documents for Each Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jLxxcVZILKU8"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"papers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2l7eKMuzLQzQ"
   },
   "outputs": [],
   "source": [
    "data = data.sort_values(by=[\"year\"], kind='mergesort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>11</td>\n",
       "      <td>1987</td>\n",
       "      <td>Microelectronic Implementations of Connectioni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11-microelectronic-implementations-of-connecti...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>515\\n\\nMICROELECTRONIC IMPLEMENTATIONS OF CONN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>12</td>\n",
       "      <td>1987</td>\n",
       "      <td>Using Neural Networks to Improve Cochlear Impl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12-using-neural-networks-to-improve-cochlear-i...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>783\\n\\nUSING NEURAL NETWORKS TO IMPROVE\\nCOCHL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>13</td>\n",
       "      <td>1987</td>\n",
       "      <td>Temporal Patterns of Activity in Neural Networks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-temporal-patterns-of-activity-in-neural-net...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>297\\n\\nTEMPORAL PATTERNS OF ACTIVITY IN\\nNEURA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6943</th>\n",
       "      <td>7280</td>\n",
       "      <td>2017</td>\n",
       "      <td>On Separability of Loss Functions, and Revisit...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>7280-on-separability-of-loss-functions-and-rev...</td>\n",
       "      <td>We revisit the classical analysis of generativ...</td>\n",
       "      <td>On Separability of Loss Functions, and Revisit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6944</th>\n",
       "      <td>7281</td>\n",
       "      <td>2017</td>\n",
       "      <td>Maxing and Ranking with Few Assumptions</td>\n",
       "      <td>Poster</td>\n",
       "      <td>7281-maxing-and-ranking-with-few-assumptions.pdf</td>\n",
       "      <td>PAC maximum                                   ...</td>\n",
       "      <td>Maxing and Ranking with Few Assumptions\\nMoein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6945</th>\n",
       "      <td>7282</td>\n",
       "      <td>2017</td>\n",
       "      <td>On clustering network-valued data</td>\n",
       "      <td>Poster</td>\n",
       "      <td>7282-on-clustering-network-valued-data.pdf</td>\n",
       "      <td>Community detection, which focuses on clusteri...</td>\n",
       "      <td>On clustering network-valued data\\n\\nSoumendu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6946</th>\n",
       "      <td>7283</td>\n",
       "      <td>2017</td>\n",
       "      <td>A General Framework for Robust Interactive Lea...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>7283-a-general-framework-for-robust-interactiv...</td>\n",
       "      <td>We propose a general framework for interactive...</td>\n",
       "      <td>A General Framework for Robust Interactive\\nLe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6947</th>\n",
       "      <td>7284</td>\n",
       "      <td>2017</td>\n",
       "      <td>Multi-view Matrix Factorization for Linear Dyn...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>7284-multi-view-matrix-factorization-for-linea...</td>\n",
       "      <td>We consider maximum likelihood estimation of l...</td>\n",
       "      <td>Multi-view Matrix Factorization for Linear\\nDy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7241 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  year                                              title  \\\n",
       "0        1  1987  Self-Organization of Associative Database and ...   \n",
       "1       10  1987  A Mean Field Theory of Layer IV of Visual Cort...   \n",
       "111     11  1987  Microelectronic Implementations of Connectioni...   \n",
       "219     12  1987  Using Neural Networks to Improve Cochlear Impl...   \n",
       "328     13  1987   Temporal Patterns of Activity in Neural Networks   \n",
       "...    ...   ...                                                ...   \n",
       "6943  7280  2017  On Separability of Loss Functions, and Revisit...   \n",
       "6944  7281  2017            Maxing and Ranking with Few Assumptions   \n",
       "6945  7282  2017                  On clustering network-valued data   \n",
       "6946  7283  2017  A General Framework for Robust Interactive Lea...   \n",
       "6947  7284  2017  Multi-view Matrix Factorization for Linear Dyn...   \n",
       "\n",
       "     event_type                                           pdf_name  \\\n",
       "0           NaN  1-self-organization-of-associative-database-an...   \n",
       "1           NaN  10-a-mean-field-theory-of-layer-iv-of-visual-c...   \n",
       "111         NaN  11-microelectronic-implementations-of-connecti...   \n",
       "219         NaN  12-using-neural-networks-to-improve-cochlear-i...   \n",
       "328         NaN  13-temporal-patterns-of-activity-in-neural-net...   \n",
       "...         ...                                                ...   \n",
       "6943     Poster  7280-on-separability-of-loss-functions-and-rev...   \n",
       "6944     Poster   7281-maxing-and-ranking-with-few-assumptions.pdf   \n",
       "6945     Poster         7282-on-clustering-network-valued-data.pdf   \n",
       "6946     Poster  7283-a-general-framework-for-robust-interactiv...   \n",
       "6947     Poster  7284-multi-view-matrix-factorization-for-linea...   \n",
       "\n",
       "                                               abstract  \\\n",
       "0                                      Abstract Missing   \n",
       "1                                      Abstract Missing   \n",
       "111                                    Abstract Missing   \n",
       "219                                    Abstract Missing   \n",
       "328                                    Abstract Missing   \n",
       "...                                                 ...   \n",
       "6943  We revisit the classical analysis of generativ...   \n",
       "6944  PAC maximum                                   ...   \n",
       "6945  Community detection, which focuses on clusteri...   \n",
       "6946  We propose a general framework for interactive...   \n",
       "6947  We consider maximum likelihood estimation of l...   \n",
       "\n",
       "                                             paper_text  \n",
       "0     767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1     683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "111   515\\n\\nMICROELECTRONIC IMPLEMENTATIONS OF CONN...  \n",
       "219   783\\n\\nUSING NEURAL NETWORKS TO IMPROVE\\nCOCHL...  \n",
       "328   297\\n\\nTEMPORAL PATTERNS OF ACTIVITY IN\\nNEURA...  \n",
       "...                                                 ...  \n",
       "6943  On Separability of Loss Functions, and Revisit...  \n",
       "6944  Maxing and Ranking with Few Assumptions\\nMoein...  \n",
       "6945  On clustering network-valued data\\n\\nSoumendu ...  \n",
       "6946  A General Framework for Robust Interactive\\nLe...  \n",
       "6947  Multi-view Matrix Factorization for Linear\\nDy...  \n",
       "\n",
       "[7241 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GG82lAG-LTHM"
   },
   "outputs": [],
   "source": [
    "years = list(data[\"year\"].unique())\n",
    "counts = list(data[\"year\"].value_counts())[::-1]\n",
    "for i in range(1,len(counts)):\n",
    "  counts[i] += counts[i-1]\n",
    "year_index = {years[i]:counts[i] for i in range(len(years))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dMSZIaUhRHgK",
    "outputId": "c9988150-618a-4e4c-f8b9-aad2953134d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1987: 90, 1988: 184, 1989: 285, 1990: 412, 1991: 552, 1992: 695, 1993: 839, 1994: 989, 1995: 1139, 1996: 1290, 1997: 1442, 1998: 1594, 1999: 1746, 2000: 1904, 2001: 2101, 2002: 2299, 2003: 2503, 2004: 2710, 2005: 2917, 2006: 3124, 2007: 3341, 2008: 3591, 2009: 3853, 2010: 4145, 2011: 4451, 2012: 4811, 2013: 5179, 2014: 5582, 2015: 5993, 2016: 6562, 2017: 7241}\n",
      "1988 94\n",
      "1989 101\n",
      "1990 127\n",
      "1991 140\n",
      "1992 143\n",
      "1993 144\n",
      "1994 150\n",
      "1995 150\n",
      "1996 151\n",
      "1997 152\n",
      "1998 152\n",
      "1999 152\n",
      "2000 158\n",
      "2001 197\n",
      "2002 198\n",
      "2003 204\n",
      "2004 207\n",
      "2005 207\n",
      "2006 207\n",
      "2007 217\n",
      "2008 250\n",
      "2009 262\n",
      "2010 292\n",
      "2011 306\n",
      "2012 360\n",
      "2013 368\n",
      "2014 403\n",
      "2015 411\n",
      "2016 569\n",
      "2017 679\n"
     ]
    }
   ],
   "source": [
    "print(year_index)\n",
    "for i in range(1988,1987+len(year_index)):\n",
    "    print(i, year_index[i]-year_index[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUjvC2BluF1s"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "$\\tt cleaned.txt$ is a txt file with all NIPS papers with all stop words and words with $\\rm{length} \\leq 3$ removed.\n",
    "\n",
    "Preprocess incorporates tokenization (splitting sentences into words), creating a vocabulary to save mappings from tokens to integer indices, and generating skip-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_fTVmdGwuHrv"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned.txt\")\n",
    "df = df[df[\"stop_removed_paper_text\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index paper_text\n",
      "0   3341        NaN\n",
      "10  3591        NaN\n",
      "20  3853        NaN\n",
      "30  4145        NaN\n",
      "40  4451        NaN\n",
      "..   ...        ...\n",
      "59  4820        NaN\n",
      "69  5188        NaN\n",
      "79  5591        NaN\n",
      "89  6002        NaN\n",
      "99  6571        NaN\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Aggregate 100 papers based on groupings of 10 from the years 2008-2017\n",
    "aggregate_data = pd.DataFrame(index=[0], columns=['index','paper_text'])\n",
    "\n",
    "offset_2008 = 3341\n",
    "offset_2009 = 3591\n",
    "offset_2010 = 3853\n",
    "offset_2011 = 4145\n",
    "offset_2012 = 4451\n",
    "offset_2013 = 4811\n",
    "offset_2014 = 5179\n",
    "offset_2015 = 5582\n",
    "offset_2016 = 5993\n",
    "offset_2017 = 6562\n",
    "\n",
    "offset = 0\n",
    "\n",
    "for i in range(0,10):\n",
    "    \n",
    "    aggregate_data.loc[i+0, 'index'] = offset_2008+offset\n",
    "    aggregate_data.loc[i+10, 'index'] = offset_2009+offset\n",
    "    aggregate_data.loc[i+20, 'index'] = offset_2010+offset\n",
    "    aggregate_data.loc[i+30, 'index'] = offset_2011+offset\n",
    "    aggregate_data.loc[i+40, 'index'] = offset_2012+offset\n",
    "    aggregate_data.loc[i+50, 'index'] = offset_2013+offset\n",
    "    aggregate_data.loc[i+60, 'index'] = offset_2014+offset\n",
    "    aggregate_data.loc[i+70, 'index'] = offset_2015+offset\n",
    "    aggregate_data.loc[i+80, 'index'] = offset_2016+offset\n",
    "    aggregate_data.loc[i+90, 'index'] = offset_2017+offset\n",
    "    \n",
    "    aggregate_data.loc[i+0, 'paper_text'] = df.iat[i,offset_2008+offset]\n",
    "    aggregate_data.loc[i+10, 'paper_text'] = df.iat[i,offset_2009+offset]\n",
    "    aggregate_data.loc[i+20, 'paper_text'] = df.iat[i,offset_2010+offset]\n",
    "    aggregate_data.loc[i+30, 'paper_text'] = df.iat[i,offset_2011+offset]\n",
    "    aggregate_data.loc[i+40, 'paper_text'] = df.iat[i,offset_2012+offset]\n",
    "    aggregate_data.loc[i+50, 'paper_text'] = df.iat[i,offset_2013+offset]\n",
    "    aggregate_data.loc[i+60, 'paper_text'] = df.iat[i,offset_2014+offset]\n",
    "    aggregate_data.loc[i+70, 'paper_text'] = df.iat[i,offset_2015+offset]\n",
    "    aggregate_data.loc[i+80, 'paper_text'] = df.iat[i,offset_2016+offset]\n",
    "    aggregate_data.loc[i+90, 'paper_text'] = df.iat[i,offset_2017+offset]\n",
    "    \n",
    "    offset += 1\n",
    "    \n",
    "\n",
    "print(aggregate_data)\n",
    "aggregate_data.to_excel(\"test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1FFPTwywuXJe"
   },
   "outputs": [],
   "source": [
    "# Initialize a preprocessor\n",
    "P = nlppipe.Preprocessor(aggregate_data, \"paper_text\", max_features=30000, maxlen=10000, min_count=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jeW0xp3dvDyo",
    "outputId": "67e5aeda-9720-445d-e4b9-a927784d1845"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a6295cf70ef1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run the preprocessing on your dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\Comp-755-Project\\Lda2vec-Tensorflow-master\\lda2vec\\nlppipe.py\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize_and_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_supplemental_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_skipgrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\Comp-755-Project\\Lda2vec-Tensorflow-master\\lda2vec\\nlppipe.py\u001b[0m in \u001b[0;36mtokenize_and_process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# Do some quick cleaning of the texts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;31m# Get number of docs supplied\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\Comp-755-Project\\Lda2vec-Tensorflow-master\\lda2vec\\nlppipe.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# Do some quick cleaning of the texts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;31m# Get number of docs supplied\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\Comp-755-Project\\Lda2vec-Tensorflow-master\\lda2vec\\nlppipe.py\u001b[0m in \u001b[0;36mclean\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtokenize_and_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# Run the preprocessing on your dataframe\n",
    "P.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should we load pretrained embeddings from file\n",
    "load_embeds = True\n",
    "\n",
    "# Load embeddings from file if we choose to do so\n",
    "if load_embeds:\n",
    "    # Load embedding matrix from file path - change path to where you saved them\n",
    "    embedding_matrix = P.load_glove(\"glove.6B.300d.txt\")\n",
    "else:\n",
    "    embedding_matrix = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7y2VxT6kvHKH"
   },
   "outputs": [],
   "source": [
    "# Save data to data_dir\n",
    "P.save_data(\"clean_data\", embedding_matrix=embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzdVCxyOSd-s"
   },
   "source": [
    "## Using the LDA2Vec Model\n",
    "\n",
    "Using the LDA2Vec model on preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tjWK7FiSgw7"
   },
   "outputs": [],
   "source": [
    "# Path to preprocessed data\n",
    "data_path  = \"clean_data\"\n",
    "# Whether or not to load saved embeddings file\n",
    "load_embeds = True\n",
    "\n",
    "# Load data from files\n",
    "(idx_to_word, word_to_idx, freqs, pivot_ids,\n",
    " target_ids, doc_ids, embed_matrix) = utils.load_preprocessed_data(data_path, load_embed_matrix=load_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EtoUS_dSj6E"
   },
   "outputs": [],
   "source": [
    "# Number of unique documents\n",
    "num_docs = doc_ids.max() + 1\n",
    "# Number of unique words in vocabulary (int)\n",
    "vocab_size = len(freqs)\n",
    "# Embed layer dimension size\n",
    "# If not loading embeds, change 128 to whatever size you want.\n",
    "embed_size = embed_matrix.shape[1] if load_embeds else 128\n",
    "# Number of topics to cluster into\n",
    "num_topics = 20\n",
    "# Amount of iterations over entire dataset\n",
    "num_epochs = 50\n",
    "# Batch size - Increase/decrease depending on memory usage\n",
    "batch_size = 4096\n",
    "# Epoch that we want to \"switch off\" regularization\n",
    "switch_loss_epoch = 0\n",
    "# Pretrained embeddings value\n",
    "pretrained_embeddings = embed_matrix if load_embeds else None\n",
    "# If True, save logdir, otherwise don't\n",
    "save_graph = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "Qx68KWfkS6sd",
    "outputId": "9a122837-2fac-49a4-ad33-8a5ff67f85c1"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "m = model(num_docs,\n",
    "          vocab_size,\n",
    "          num_topics,\n",
    "          embedding_size=embed_size,\n",
    "          pretrained_embeddings=pretrained_embeddings,\n",
    "          freqs=freqs,\n",
    "          batch_size = batch_size,\n",
    "          save_graph_def=save_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uAHda-IRTQvW",
    "outputId": "ef865d92-1c10-4999-c69a-7392dc283926",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "m.train(pivot_ids,\n",
    "        target_ids,\n",
    "        doc_ids,\n",
    "        len(pivot_ids),\n",
    "        num_epochs,\n",
    "        idx_to_word=idx_to_word,\n",
    "        switch_loss_epoch=switch_loss_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzdVCxyOSd-s"
   },
   "source": [
    "## Get Word and Topic Embeddings\n",
    "\n",
    "Visualize topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2itPKBHwEmyf",
    "outputId": "8e1667af-ee91-4a94-c9b1-ffde52293b44",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6TmdHynobsp",
    "outputId": "9ef19df5-670d-45bd-b5fc-f50d95783c9c"
   },
   "outputs": [],
   "source": [
    "doc_embed = m.sesh.run(m.mixture.doc_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2kJaNyaEskRC",
    "outputId": "2ec3c927-0e8c-4f24-bc35-ba2851be7da0"
   },
   "outputs": [],
   "source": [
    "topic_embed = m.sesh.run(m.mixture.topic_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ioyv6Ju4vZ-v"
   },
   "outputs": [],
   "source": [
    "word_embed = m.sesh.run(m.w_embed.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique words in order of index 0-vocab_size\n",
    "vocabulary = []\n",
    "for k,v in idx_to_word.items():\n",
    "    vocabulary.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = np.load(\"clean_data\" + \"/doc_lengths.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = utils.prepare_topics(doc_embed, topic_embed, word_embed, np.array(vocabulary), doc_lengths=doc_lengths,\n",
    "                              term_frequency=freqs, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_vis_data = pyLDAvis.prepare(**vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(prepared_vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"doc_embed\", doc_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"word_embed\", word_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"topic_embed\", topic_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(embed_idxs, embed_type):\n",
    "    if embed_type == 0:  # Topics\n",
    "        norm_embeds = topic_embed / np.linalg.norm(topic_embed, axis=1).reshape((topic_embed.shape[0],1))\n",
    "        embed_vec = topic_embed[embed_idxs[2]] - topic_embed[embed_idxs[0]] + topic_embed[embed_idxs[1]]\n",
    "        embed_norm = (embed_vec) / np.linalg.norm(embed_vec)\n",
    "    else:  # Words\n",
    "        norm_embeds = word_embed / np.linalg.norm(word_embed, axis=1).reshape((word_embed.shape[0],1))\n",
    "        embed_vec = word_embed[embed_idxs[2]] - word_embed[embed_idxs[0]] + word_embed[embed_idxs[1]]\n",
    "        embed_norm = (embed_vec) / np.linalg.norm(embed_vec)\n",
    "    cos_sim = np.dot(norm_embeds, embed_norm)\n",
    "    sort = (-cos_sim).argsort()\n",
    "    for i in range(3): sort=sort[sort != embed_idxs[i]]\n",
    "    return sort[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosadd(embed1, embed2, embed3, embed4, embed_type):\n",
    "    if embed_type == 0:\n",
    "        norm_embed1 = topic_embed[embed1] / np.linalg.norm(topic_embed[embed1])\n",
    "        norm_embed2 = topic_embed[embed2] / np.linalg.norm(topic_embed[embed2])\n",
    "        norm_embed3 = topic_embed[embed3] / np.linalg.norm(topic_embed[embed3])\n",
    "        norm_embed4 = topic_embed[embed4] / np.linalg.norm(topic_embed[embed4])\n",
    "    else:\n",
    "        norm_embed1 = word_embed[embed1] / np.linalg.norm(word_embed[embed1])\n",
    "        norm_embed2 = word_embed[embed2] / np.linalg.norm(word_embed[embed2])\n",
    "        norm_embed3 = word_embed[embed3] / np.linalg.norm(word_embed[embed3])\n",
    "        norm_embed4 = word_embed[embed4] / np.linalg.norm(word_embed[embed4])\n",
    "    return np.dot(norm_embed4, norm_embed3) - np.dot(norm_embed4, norm_embed1) + np.dot(norm_embed4, norm_embed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import *\n",
    "from math import comb\n",
    "combine = list(combinations(list(range(word_embed.shape[0])), 3))\n",
    "score = np.zeros(comb(word_embed.shape[0],3))\n",
    "for i in range(score.shape[0]):\n",
    "    (embed_idx1, embed_idx2, embed_idx3) = combine[i]\n",
    "    score[i] = cosadd(embed_idx1, embed_idx2, embed_idx3, closest([embed_idx1, embed_idx2, embed_idx3], 1), 1)\n",
    "top_analogies = [list(combine[idx])+[closest([combine[idx][0], combine[idx][1], combine[idx][2]], 1)] for idx in (-score).argsort()]\n",
    "top_scores = [score[idx] for idx in (-score).argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_analogies[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scores[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosadd(10, 14, 33, 167, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.sesh.get_k_closest([], in_type='word', vs_type='word', k=10, idx_to_word=None, verbose=False):"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LDA2Vec",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

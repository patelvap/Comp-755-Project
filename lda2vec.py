# -*- coding: utf-8 -*-
"""LDA2Vec

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XNZ7KwdNkW9d23bWNRoImGs8yXDj6nyQ

# LDA2Vec
Adapted from tensorflow implementation: 

(New) https://github.com/nateraw/Lda2vec-Tensorflow

(Old) https://github.com/meereeum/lda2vec-tf

Import Libraries (Must restart after installing pyLDAvis)
"""

!pip install pyLDAvis

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

# %tensorflow_version 2.x
import tensorflow as tf
from tensorflow.keras import layers

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

from google.colab import drive
#drive.mount('/content/drive')
drive.mount("/content/drive", force_remount=True)

!cp -r /content/drive/MyDrive/Project/Lda2vec-Tensorflow-master .
!cp /content/drive/MyDrive/Project/cleaned.txt .
!cp /content/drive/MyDrive/Project/papers.csv .

import sys
sys.path.append('/content/Lda2vec-Tensorflow-master')

from lda2vec import *

"""## Find Number of Documents with Each Year"""

data = pd.read_csv("papers.csv")

data = data.sort_values(by=["year"], kind='mergesort')

years = list(data["year"].unique())
counts = list(data["year"].value_counts())[::-1]
for i in range(1,len(counts)):
  counts[i] += counts[i-1]
year_index = {years[i]:counts[i] for i in range(len(years))}

year_index

"""## Preprocessing"""

df = pd.read_csv("cleaned.txt")
df = df[df["stop_removed_paper_text"].notnull()]

df = df[6562:7241] # Use only 2017 papers

# Initialize a preprocessor
P = nlppipe.Preprocessor(df, "stop_removed_paper_text", max_features=30000, maxlen=10000, min_count=30)

# Run the preprocessing on your dataframe
P.preprocess()

# Save data to data_dir
P.save_data("clean_data", embedding_matrix=None)

"""## Using the LDA2Vec Model

Use separate model if you wish to test on other sets of documents
"""

# Path to preprocessed data
data_path  = "clean_data"
# Whether or not to load saved embeddings file
load_embeds = True

# Load data from files
(idx_to_word, word_to_idx, freqs, pivot_ids,
 target_ids, doc_ids) = utils.load_preprocessed_data(data_path, load_embed_matrix=False)

# Number of unique documents
num_docs = doc_ids.max() + 1
# Number of unique words in vocabulary (int)
vocab_size = len(freqs)
# Number of topics to cluster into
num_topics = 20
# Amount of iterations over entire dataset
num_epochs = 100
# Batch size - Increase/decrease depending on memory usage
batch_size = 4096
# Epoch that we want to "switch on" LDA loss
switch_loss_epoch = 0
# If True, save logdir, otherwise don't
save_graph = True

# Initialize the model
m = model(num_docs,
          vocab_size,
          num_topics,
          embedding_size=128,
          pretrained_embeddings=None,
          freqs=freqs,
          batch_size = batch_size,
          save_graph_def=save_graph)

# Train the model
m.train(pivot_ids,
        target_ids,
        doc_ids,
        len(pivot_ids),
        num_epochs,
        idx_to_word=idx_to_word,
        switch_loss_epoch=switch_loss_epoch)


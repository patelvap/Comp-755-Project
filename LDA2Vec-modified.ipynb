{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eS5jhdJbp98C"
   },
   "source": [
    "# LDA2Vec\n",
    "Adapted from tensorflow implementation: \n",
    "\n",
    "(Current) https://github.com/nateraw/Lda2vec-Tensorflow\n",
    "\n",
    "(Old) https://github.com/meereeum/lda2vec-tf\n",
    "\n",
    "(Original) https://github.com/cemoody/lda2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1HBXFOPqJOj",
    "outputId": "6ee5f2d9-9b99-4dfc-f470-d11f82c38409"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyLDAvis\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sfl9g1ze76ZO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./Lda2vec-Tensorflow-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-Apc0J1AqXEb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\matth\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from lda2vec import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-193a12293e99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'/device:GPU:0'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU device not found'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Found GPU at: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vc9Dtw23LCZW"
   },
   "source": [
    "## Find Ending Index of Documents for Each Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jLxxcVZILKU8"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"papers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2l7eKMuzLQzQ"
   },
   "outputs": [],
   "source": [
    "data = data.sort_values(by=[\"year\"], kind='mergesort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>11</td>\n",
       "      <td>1987</td>\n",
       "      <td>Microelectronic Implementations of Connectioni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11-microelectronic-implementations-of-connecti...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>515\\n\\nMICROELECTRONIC IMPLEMENTATIONS OF CONN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>12</td>\n",
       "      <td>1987</td>\n",
       "      <td>Using Neural Networks to Improve Cochlear Impl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12-using-neural-networks-to-improve-cochlear-i...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>783\\n\\nUSING NEURAL NETWORKS TO IMPROVE\\nCOCHL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>13</td>\n",
       "      <td>1987</td>\n",
       "      <td>Temporal Patterns of Activity in Neural Networks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-temporal-patterns-of-activity-in-neural-net...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>297\\n\\nTEMPORAL PATTERNS OF ACTIVITY IN\\nNEURA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6943</th>\n",
       "      <td>7280</td>\n",
       "      <td>2017</td>\n",
       "      <td>On Separability of Loss Functions, and Revisit...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>7280-on-separability-of-loss-functions-and-rev...</td>\n",
       "      <td>We revisit the classical analysis of generativ...</td>\n",
       "      <td>On Separability of Loss Functions, and Revisit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6944</th>\n",
       "      <td>7281</td>\n",
       "      <td>2017</td>\n",
       "      <td>Maxing and Ranking with Few Assumptions</td>\n",
       "      <td>Poster</td>\n",
       "      <td>7281-maxing-and-ranking-with-few-assumptions.pdf</td>\n",
       "      <td>PAC maximum                                   ...</td>\n",
       "      <td>Maxing and Ranking with Few Assumptions\\nMoein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6945</th>\n",
       "      <td>7282</td>\n",
       "      <td>2017</td>\n",
       "      <td>On clustering network-valued data</td>\n",
       "      <td>Poster</td>\n",
       "      <td>7282-on-clustering-network-valued-data.pdf</td>\n",
       "      <td>Community detection, which focuses on clusteri...</td>\n",
       "      <td>On clustering network-valued data\\n\\nSoumendu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6946</th>\n",
       "      <td>7283</td>\n",
       "      <td>2017</td>\n",
       "      <td>A General Framework for Robust Interactive Lea...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>7283-a-general-framework-for-robust-interactiv...</td>\n",
       "      <td>We propose a general framework for interactive...</td>\n",
       "      <td>A General Framework for Robust Interactive\\nLe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6947</th>\n",
       "      <td>7284</td>\n",
       "      <td>2017</td>\n",
       "      <td>Multi-view Matrix Factorization for Linear Dyn...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>7284-multi-view-matrix-factorization-for-linea...</td>\n",
       "      <td>We consider maximum likelihood estimation of l...</td>\n",
       "      <td>Multi-view Matrix Factorization for Linear\\nDy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7241 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  year                                              title  \\\n",
       "0        1  1987  Self-Organization of Associative Database and ...   \n",
       "1       10  1987  A Mean Field Theory of Layer IV of Visual Cort...   \n",
       "111     11  1987  Microelectronic Implementations of Connectioni...   \n",
       "219     12  1987  Using Neural Networks to Improve Cochlear Impl...   \n",
       "328     13  1987   Temporal Patterns of Activity in Neural Networks   \n",
       "...    ...   ...                                                ...   \n",
       "6943  7280  2017  On Separability of Loss Functions, and Revisit...   \n",
       "6944  7281  2017            Maxing and Ranking with Few Assumptions   \n",
       "6945  7282  2017                  On clustering network-valued data   \n",
       "6946  7283  2017  A General Framework for Robust Interactive Lea...   \n",
       "6947  7284  2017  Multi-view Matrix Factorization for Linear Dyn...   \n",
       "\n",
       "     event_type                                           pdf_name  \\\n",
       "0           NaN  1-self-organization-of-associative-database-an...   \n",
       "1           NaN  10-a-mean-field-theory-of-layer-iv-of-visual-c...   \n",
       "111         NaN  11-microelectronic-implementations-of-connecti...   \n",
       "219         NaN  12-using-neural-networks-to-improve-cochlear-i...   \n",
       "328         NaN  13-temporal-patterns-of-activity-in-neural-net...   \n",
       "...         ...                                                ...   \n",
       "6943     Poster  7280-on-separability-of-loss-functions-and-rev...   \n",
       "6944     Poster   7281-maxing-and-ranking-with-few-assumptions.pdf   \n",
       "6945     Poster         7282-on-clustering-network-valued-data.pdf   \n",
       "6946     Poster  7283-a-general-framework-for-robust-interactiv...   \n",
       "6947     Poster  7284-multi-view-matrix-factorization-for-linea...   \n",
       "\n",
       "                                               abstract  \\\n",
       "0                                      Abstract Missing   \n",
       "1                                      Abstract Missing   \n",
       "111                                    Abstract Missing   \n",
       "219                                    Abstract Missing   \n",
       "328                                    Abstract Missing   \n",
       "...                                                 ...   \n",
       "6943  We revisit the classical analysis of generativ...   \n",
       "6944  PAC maximum                                   ...   \n",
       "6945  Community detection, which focuses on clusteri...   \n",
       "6946  We propose a general framework for interactive...   \n",
       "6947  We consider maximum likelihood estimation of l...   \n",
       "\n",
       "                                             paper_text  \n",
       "0     767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1     683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "111   515\\n\\nMICROELECTRONIC IMPLEMENTATIONS OF CONN...  \n",
       "219   783\\n\\nUSING NEURAL NETWORKS TO IMPROVE\\nCOCHL...  \n",
       "328   297\\n\\nTEMPORAL PATTERNS OF ACTIVITY IN\\nNEURA...  \n",
       "...                                                 ...  \n",
       "6943  On Separability of Loss Functions, and Revisit...  \n",
       "6944  Maxing and Ranking with Few Assumptions\\nMoein...  \n",
       "6945  On clustering network-valued data\\n\\nSoumendu ...  \n",
       "6946  A General Framework for Robust Interactive\\nLe...  \n",
       "6947  Multi-view Matrix Factorization for Linear\\nDy...  \n",
       "\n",
       "[7241 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GG82lAG-LTHM"
   },
   "outputs": [],
   "source": [
    "years = list(data[\"year\"].unique())\n",
    "counts = list(data[\"year\"].value_counts())[::-1]\n",
    "for i in range(1,len(counts)):\n",
    "  counts[i] += counts[i-1]\n",
    "year_index = {years[i]:counts[i] for i in range(len(years))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dMSZIaUhRHgK",
    "outputId": "c9988150-618a-4e4c-f8b9-aad2953134d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1987: 90, 1988: 184, 1989: 285, 1990: 412, 1991: 552, 1992: 695, 1993: 839, 1994: 989, 1995: 1139, 1996: 1290, 1997: 1442, 1998: 1594, 1999: 1746, 2000: 1904, 2001: 2101, 2002: 2299, 2003: 2503, 2004: 2710, 2005: 2917, 2006: 3124, 2007: 3341, 2008: 3591, 2009: 3853, 2010: 4145, 2011: 4451, 2012: 4811, 2013: 5179, 2014: 5582, 2015: 5993, 2016: 6562, 2017: 7241}\n",
      "1988 94\n",
      "1989 101\n",
      "1990 127\n",
      "1991 140\n",
      "1992 143\n",
      "1993 144\n",
      "1994 150\n",
      "1995 150\n",
      "1996 151\n",
      "1997 152\n",
      "1998 152\n",
      "1999 152\n",
      "2000 158\n",
      "2001 197\n",
      "2002 198\n",
      "2003 204\n",
      "2004 207\n",
      "2005 207\n",
      "2006 207\n",
      "2007 217\n",
      "2008 250\n",
      "2009 262\n",
      "2010 292\n",
      "2011 306\n",
      "2012 360\n",
      "2013 368\n",
      "2014 403\n",
      "2015 411\n",
      "2016 569\n",
      "2017 679\n"
     ]
    }
   ],
   "source": [
    "print(year_index)\n",
    "for i in range(1988,1987+len(year_index)):\n",
    "    print(i, year_index[i]-year_index[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUjvC2BluF1s"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "$\\tt cleaned.txt$ is a txt file with all NIPS papers with all stop words and words with $\\rm{length} \\leq 3$ removed.\n",
    "\n",
    "Preprocess incorporates tokenization (splitting sentences into words), creating a vocabulary to save mappings from tokens to integer indices, and generating skip-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_fTVmdGwuHrv"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleaned.txt\")\n",
    "df = df[df[\"stop_removed_paper_text\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   group number                                         paper_text\n",
      "0             0  self organization associative database applica...\n",
      "1             1  invariant object recognition using distributed...\n",
      "2             2  massively parallel self tuning context free pa...\n",
      "3             3  satyanarayana tsividis graf reconfigurable ana...\n",
      "4             4  evaluation adaptive mixtures competing experts...\n",
      "..          ...                                                ...\n",
      "89           89  rotting bandits levine electrical engineering ...\n",
      "90           90  conic scan cover algorithms nonparametric topi...\n",
      "91           91  query complexity clustering side information a...\n",
      "92           92  plan attend generate planning sequence sequenc...\n",
      "93           93  boltzmann exploration done right nicol cesa bi...\n",
      "\n",
      "[94 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#df length is 7238, so Divisors of number 7238: 1, 2, 7, 11, 14, 22, 47, 77, 94, 154, 329, 517, 658, 1034, 3619, 7238\n",
    "#Will do 94 groups of 77 papers back-to-back. The current df has already sorted them by id. Therefore we can assume they are\n",
    "#chronologically ordered. Another naive way to do this is just do the groupings by year in a non-uniform manner.\n",
    "aggregate_data = pd.DataFrame(index=[0], columns=['group number','paper_text'])\n",
    "\n",
    "start_index = 0\n",
    "temp = start_index\n",
    "for counter in range(0,94):\n",
    "    aggregate_data.loc[counter, 'group number'] = counter\n",
    "    aggregate_string = ''\n",
    "    for i in range(start_index,start_index+77):\n",
    "        paper_text = df.iat[i,1]\n",
    "        aggregate_string += paper_text\n",
    "        #print(\"Paper #\",i)\n",
    "    aggregate_data.loc[counter, 'paper_text'] = aggregate_string\n",
    "    temp = i\n",
    "    start_index = temp+1\n",
    "    \n",
    "    #print(\"Temp: \", temp)\n",
    "    \n",
    "print(aggregate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1FFPTwywuXJe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\anaconda3\\lib\\site-packages\\spacy\\util.py:841: UserWarning: [W094] Model 'en_core_web_sm' (2.2.0) specifies an under-constrained spaCy version requirement: >=2.2.0. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the \"spacy_version\" in your meta.json to a version range, with a lower and upper pin. For example: >=3.2.0,<3.3.0\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E053] Could not read config.cfg from C:\\Users\\matth\\anaconda3\\lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.2.0\\config.cfg",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8931f85a585c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Initialize a preprocessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlppipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maggregate_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"paper_text\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Documents\\GitHub\\Comp-755-Project\\Lda2vec-Tensorflow-master\\lda2vec\\nlppipe.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, df, textcol, max_features, maxlen, window_size, nlp, bad, token_type, min_count)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# Here we disable parts of spacy's pipeline - REALLY improves speed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ner'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'parser'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         self.nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS,\n\u001b[0;32m     43\u001b[0m                                                        spacy.attrs.IS_STOP)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \"\"\"\n\u001b[1;32m---> 51\u001b[1;33m     return util.load_model(\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    418\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"blank:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# installed as package\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# path to model data directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \"\"\"\n\u001b[0;32m    452\u001b[0m     \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\en_core_web_sm\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m     return load_model_from_path(\n\u001b[0m\u001b[0;32m    616\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[0mconfig_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m\"config.cfg\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[0moverrides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model_from_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_config\u001b[1;34m(path, overrides, interpolate)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE053\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"config.cfg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m         return config.from_disk(\n\u001b[0;32m    648\u001b[0m             \u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E053] Could not read config.cfg from C:\\Users\\matth\\anaconda3\\lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.2.0\\config.cfg"
     ]
    }
   ],
   "source": [
    "# Initialize a preprocessor\n",
    "P = nlppipe.Preprocessor(aggregate_data, \"paper_text\", max_features=30000, maxlen=10000, min_count=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jeW0xp3dvDyo",
    "outputId": "67e5aeda-9720-445d-e4b9-a927784d1845"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'P' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a6295cf70ef1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run the preprocessing on your dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'P' is not defined"
     ]
    }
   ],
   "source": [
    "# Run the preprocessing on your dataframe\n",
    "P.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should we load pretrained embeddings from file\n",
    "load_embeds = True\n",
    "\n",
    "# Load embeddings from file if we choose to do so\n",
    "if load_embeds:\n",
    "    # Load embedding matrix from file path - change path to where you saved them\n",
    "    embedding_matrix = P.load_glove(\"glove.6B.300d.txt\")\n",
    "else:\n",
    "    embedding_matrix = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7y2VxT6kvHKH"
   },
   "outputs": [],
   "source": [
    "# Save data to data_dir\n",
    "P.save_data(\"clean_data\", embedding_matrix=embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzdVCxyOSd-s"
   },
   "source": [
    "## Using the LDA2Vec Model\n",
    "\n",
    "Using the LDA2Vec model on preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tjWK7FiSgw7"
   },
   "outputs": [],
   "source": [
    "# Path to preprocessed data\n",
    "data_path  = \"clean_data\"\n",
    "# Whether or not to load saved embeddings file\n",
    "load_embeds = True\n",
    "\n",
    "# Load data from files\n",
    "(idx_to_word, word_to_idx, freqs, pivot_ids,\n",
    " target_ids, doc_ids, embed_matrix) = utils.load_preprocessed_data(data_path, load_embed_matrix=load_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EtoUS_dSj6E"
   },
   "outputs": [],
   "source": [
    "# Number of unique documents\n",
    "num_docs = doc_ids.max() + 1\n",
    "# Number of unique words in vocabulary (int)\n",
    "vocab_size = len(freqs)\n",
    "# Embed layer dimension size\n",
    "# If not loading embeds, change 128 to whatever size you want.\n",
    "embed_size = embed_matrix.shape[1] if load_embeds else 128\n",
    "# Number of topics to cluster into\n",
    "num_topics = 20\n",
    "# Amount of iterations over entire dataset\n",
    "num_epochs = 50\n",
    "# Batch size - Increase/decrease depending on memory usage\n",
    "batch_size = 4096\n",
    "# Epoch that we want to \"switch off\" regularization\n",
    "switch_loss_epoch = 0\n",
    "# Pretrained embeddings value\n",
    "pretrained_embeddings = embed_matrix if load_embeds else None\n",
    "# If True, save logdir, otherwise don't\n",
    "save_graph = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "Qx68KWfkS6sd",
    "outputId": "9a122837-2fac-49a4-ad33-8a5ff67f85c1"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "m = model(num_docs,\n",
    "          vocab_size,\n",
    "          num_topics,\n",
    "          embedding_size=embed_size,\n",
    "          pretrained_embeddings=pretrained_embeddings,\n",
    "          freqs=freqs,\n",
    "          batch_size = batch_size,\n",
    "          save_graph_def=save_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uAHda-IRTQvW",
    "outputId": "ef865d92-1c10-4999-c69a-7392dc283926",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "m.train(pivot_ids,\n",
    "        target_ids,\n",
    "        doc_ids,\n",
    "        len(pivot_ids),\n",
    "        num_epochs,\n",
    "        idx_to_word=idx_to_word,\n",
    "        switch_loss_epoch=switch_loss_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzdVCxyOSd-s"
   },
   "source": [
    "## Get Word and Topic Embeddings\n",
    "\n",
    "Visualize topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2itPKBHwEmyf",
    "outputId": "8e1667af-ee91-4a94-c9b1-ffde52293b44",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6TmdHynobsp",
    "outputId": "9ef19df5-670d-45bd-b5fc-f50d95783c9c"
   },
   "outputs": [],
   "source": [
    "doc_embed = m.sesh.run(m.mixture.doc_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2kJaNyaEskRC",
    "outputId": "2ec3c927-0e8c-4f24-bc35-ba2851be7da0"
   },
   "outputs": [],
   "source": [
    "topic_embed = m.sesh.run(m.mixture.topic_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ioyv6Ju4vZ-v"
   },
   "outputs": [],
   "source": [
    "word_embed = m.sesh.run(m.w_embed.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique words in order of index 0-vocab_size\n",
    "vocabulary = []\n",
    "for k,v in idx_to_word.items():\n",
    "    vocabulary.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = np.load(\"clean_data\" + \"/doc_lengths.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = utils.prepare_topics(doc_embed, topic_embed, word_embed, np.array(vocabulary), doc_lengths=doc_lengths,\n",
    "                              term_frequency=freqs, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_vis_data = pyLDAvis.prepare(**vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(prepared_vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"doc_embed\", doc_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"word_embed\", word_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"topic_embed\", topic_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(embed_idxs, embed_type):\n",
    "    if embed_type == 0:  # Topics\n",
    "        norm_embeds = topic_embed / np.linalg.norm(topic_embed, axis=1).reshape((topic_embed.shape[0],1))\n",
    "        embed_vec = topic_embed[embed_idxs[2]] - topic_embed[embed_idxs[0]] + topic_embed[embed_idxs[1]]\n",
    "        embed_norm = (embed_vec) / np.linalg.norm(embed_vec)\n",
    "    else:  # Words\n",
    "        norm_embeds = word_embed / np.linalg.norm(word_embed, axis=1).reshape((word_embed.shape[0],1))\n",
    "        embed_vec = word_embed[embed_idxs[2]] - word_embed[embed_idxs[0]] + word_embed[embed_idxs[1]]\n",
    "        embed_norm = (embed_vec) / np.linalg.norm(embed_vec)\n",
    "    cos_sim = np.dot(norm_embeds, embed_norm)\n",
    "    sort = (-cos_sim).argsort()\n",
    "    for i in range(3): sort=sort[sort != embed_idxs[i]]\n",
    "    return sort[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosadd(embed1, embed2, embed3, embed4, embed_type):\n",
    "    if embed_type == 0:\n",
    "        norm_embed1 = topic_embed[embed1] / np.linalg.norm(topic_embed[embed1])\n",
    "        norm_embed2 = topic_embed[embed2] / np.linalg.norm(topic_embed[embed2])\n",
    "        norm_embed3 = topic_embed[embed3] / np.linalg.norm(topic_embed[embed3])\n",
    "        norm_embed4 = topic_embed[embed4] / np.linalg.norm(topic_embed[embed4])\n",
    "    else:\n",
    "        norm_embed1 = word_embed[embed1] / np.linalg.norm(word_embed[embed1])\n",
    "        norm_embed2 = word_embed[embed2] / np.linalg.norm(word_embed[embed2])\n",
    "        norm_embed3 = word_embed[embed3] / np.linalg.norm(word_embed[embed3])\n",
    "        norm_embed4 = word_embed[embed4] / np.linalg.norm(word_embed[embed4])\n",
    "    return np.dot(norm_embed4, norm_embed3) - np.dot(norm_embed4, norm_embed1) + np.dot(norm_embed4, norm_embed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import *\n",
    "from math import comb\n",
    "combine = list(combinations(list(range(word_embed.shape[0])), 3))\n",
    "score = np.zeros(comb(word_embed.shape[0],3))\n",
    "for i in range(score.shape[0]):\n",
    "    (embed_idx1, embed_idx2, embed_idx3) = combine[i]\n",
    "    score[i] = cosadd(embed_idx1, embed_idx2, embed_idx3, closest([embed_idx1, embed_idx2, embed_idx3], 1), 1)\n",
    "top_analogies = [list(combine[idx])+[closest([combine[idx][0], combine[idx][1], combine[idx][2]], 1)] for idx in (-score).argsort()]\n",
    "top_scores = [score[idx] for idx in (-score).argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_analogies[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scores[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosadd(10, 14, 33, 167, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.sesh.get_k_closest([], in_type='word', vs_type='word', k=10, idx_to_word=None, verbose=False):"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LDA2Vec",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
